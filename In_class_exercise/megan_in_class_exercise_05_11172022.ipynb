{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/megancoll/megan_INFO5731_-Spring2022/blob/main/In_class_exercise/megan_in_class_exercise_05_11172022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTx2sHxE_Dkw"
      },
      "source": [
        "# **The fifth in-class-exercise (40 points in total, 11/17/2022)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YpAqI_U_Dky"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text classification as well as the performance evaluation. In addition, you are requried to conduct *10 fold cross validation (https://scikit-learn.org/stable/modules/cross_validation.html)* in the training. \n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data. \n",
        "\n",
        "Algorithms:\n",
        "\n",
        "(1) MultinominalNB\n",
        "\n",
        "(2) SVM \n",
        "\n",
        "(3) KNN \n",
        "\n",
        "(4) Decision tree\n",
        "\n",
        "(5) Random Forest\n",
        "\n",
        "(6) XGBoost\n",
        "\n",
        "Evaluation measurement:\n",
        "\n",
        "(1) Accuracy\n",
        "\n",
        "(2) Recall\n",
        "\n",
        "(3) Precison \n",
        "\n",
        "(4) F-1 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP9qlWtz_Dkz",
        "outputId": "adbf34d6-9db1-4859-facf-ffbdae37f924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, silhouette_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import svm\n",
        "from sklearn import neighbors\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "BZn9d2yimoWn",
        "outputId": "f091f394-8fda-42f8-e493-1ac79516f75e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-28b13f42-b660-4b3a-82d5-7b967f89c282\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-28b13f42-b660-4b3a-82d5-7b967f89c282\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving exercise05_datacollection.zip to exercise05_datacollection.zip\n",
            "Archive:  /content/exercise05_datacollection.zip\n",
            "   creating: exercise09_datacollection/\n",
            "  inflating: exercise09_datacollection/stsa-test.txt  \n",
            "  inflating: exercise09_datacollection/stsa-train.txt  \n"
          ]
        }
      ],
      "source": [
        "#upload zip file and unzip\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "!unzip \"/content/exercise05_datacollection.zip\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLLaHRa4Ir6l"
      },
      "outputs": [],
      "source": [
        "#opening the training and testing files\n",
        "\n",
        "with open(\"/content/exercise09_datacollection/stsa-test.txt\") as file:\n",
        "    lines1 = [line.rstrip() for line in file]\n",
        "\n",
        "with open(\"/content/exercise09_datacollection/stsa-train.txt\") as file:\n",
        "    lines2 = [line.rstrip() for line in file]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGOhY75hhmmF"
      },
      "outputs": [],
      "source": [
        "#splitting labels from the text of the reviews\n",
        "\n",
        "x_train_text = [l[2:] for l in lines1]\n",
        "\n",
        "y_train = [int(l[0]) for l in lines1]\n",
        "\n",
        "\n",
        "x_test_text = [l[2:] for l in lines2]\n",
        "\n",
        "y_test = [int(l[0]) for l in lines2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6QFqB1siTtX"
      },
      "source": [
        "For this assignment, I will use bag of words as features. In a bag of words model, each word type in the training data is a possible feature. In this case, there are 7050 word types in the training data, so 7050 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMuju3cHh273"
      },
      "outputs": [],
      "source": [
        "#tokenizing the texts into lists of words\n",
        "\n",
        "x_train_tokenized = [word_tokenize(l) for l in x_train_text]\n",
        "x_test_tokenized = [word_tokenize(l) for l in x_test_text]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD0soW8MkVqO"
      },
      "outputs": [],
      "source": [
        "#this is to get a list of all the words in the training set\n",
        "\n",
        "all_words = []\n",
        "for l in x_train_tokenized:\n",
        "  for word in l:\n",
        "    all_words.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rQV67I4khSm",
        "outputId": "6f2ce3d1-36ad-4b68-9b30-3ae41d34c26d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7050"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "all_words = list(set(all_words))\n",
        "\n",
        "len(all_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOppKL-Ykk7C"
      },
      "outputs": [],
      "source": [
        "#turning the training/testing data into lists of features (bag-of-words features)\n",
        "\n",
        "X_train = []\n",
        "X_test = []\n",
        "for l in x_train_tokenized:\n",
        "  x_train = []\n",
        "  for word in all_words:\n",
        "    if word in l:\n",
        "      x_train.append(1)\n",
        "    else:\n",
        "      x_train.append(0)\n",
        "  x_train = np.array(x_train)\n",
        "  X_train.append(x_train)\n",
        "\n",
        "\n",
        "for l in x_test_tokenized:\n",
        "  x_test = []\n",
        "  for word in all_words:\n",
        "    if word in l:\n",
        "      x_test.append(1)\n",
        "    else:\n",
        "      x_test.append(0)\n",
        "  x_test = np.array(x_test)\n",
        "  X_test.append(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmNujMhPsZSA"
      },
      "source": [
        "Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8owgds8IoB8g",
        "outputId": "f3c4c29d-d6b5-4b4b-813a-1b71ca93b1b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.73224044 0.77472527 0.7032967  0.75274725 0.7032967  0.8021978\n",
            " 0.68131868 0.76923077 0.73626374 0.74175824]\n"
          ]
        }
      ],
      "source": [
        "#naive bayes model\n",
        "model_NB = MultinomialNB()\n",
        "scoring = ['accuracy', 'precision_macro', 'recall_macro']\n",
        "scores = cross_validate(model_NB, X_train, y_train, scoring=scoring, cv=10, return_estimator=True)\n",
        "\n",
        "print(scores['test_accuracy'])\n",
        "\n",
        "#choosing best model\n",
        "max_acc = np.argmax(scores['test_accuracy'])\n",
        "best_model = scores['estimator'][max_acc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jQoRUg4rP-6",
        "outputId": "b98aff99-baa4-4a27-d01c-87605d6bd1fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.75      0.74      3310\n",
            "           1       0.76      0.73      0.75      3610\n",
            "\n",
            "    accuracy                           0.74      6920\n",
            "   macro avg       0.74      0.74      0.74      6920\n",
            "weighted avg       0.74      0.74      0.74      6920\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#evaluating chosen model on test data\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgwSPO-XuMy7"
      },
      "outputs": [],
      "source": [
        "#turning the above code into function for reuse with different models\n",
        "\n",
        "def run_experiment(model, mod_type):\n",
        "  scoring = ['accuracy', 'precision_macro', 'recall_macro']\n",
        "  scores = cross_validate(model, X_train, y_train, scoring=scoring, cv=10, return_estimator=True)\n",
        "\n",
        "  print(f\"{mod_type}: dev accuracy from cross-validation\")\n",
        "  print(scores['test_accuracy'])\n",
        "  print(\"------------------------------------------------------------------\")\n",
        "\n",
        "  #choosing best model\n",
        "  max_acc = np.argmax(scores['test_accuracy'])\n",
        "  best_model = scores['estimator'][max_acc]\n",
        "\n",
        "  #evaluating chosen model on test data\n",
        "  y_pred = best_model.predict(X_test)\n",
        "  print(\"Test Results:\")\n",
        "  print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G10ymoJseYI"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9TCQPfJsgpC",
        "outputId": "e7b24aed-e87f-42da-e899-c5e230a99ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM: dev accuracy from cross-validation\n",
            "[0.68306011 0.78571429 0.68131868 0.68681319 0.66483516 0.70879121\n",
            " 0.66483516 0.77472527 0.76923077 0.67032967]\n",
            "------------------------------------------------------------------\n",
            "Test Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.73      0.71      3310\n",
            "           1       0.74      0.71      0.72      3610\n",
            "\n",
            "    accuracy                           0.72      6920\n",
            "   macro avg       0.72      0.72      0.72      6920\n",
            "weighted avg       0.72      0.72      0.72      6920\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_SVM = svm.LinearSVC()\n",
        "\n",
        "run_experiment(model_SVM, \"SVM\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHCGsLmBtNBU"
      },
      "source": [
        "KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Feoz1oU8tPOo",
        "outputId": "ce4d6edd-6c53-4777-f555-3f974110fa2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN: dev accuracy from cross-validation\n",
            "[0.49726776 0.55494505 0.53846154 0.56593407 0.57692308 0.5989011\n",
            " 0.48351648 0.57692308 0.60989011 0.58241758]\n",
            "------------------------------------------------------------------\n",
            "Test Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.75      0.61      3310\n",
            "           1       0.61      0.35      0.44      3610\n",
            "\n",
            "    accuracy                           0.54      6920\n",
            "   macro avg       0.56      0.55      0.53      6920\n",
            "weighted avg       0.56      0.54      0.52      6920\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_KNN = neighbors.KNeighborsClassifier()\n",
        "\n",
        "run_experiment(model_KNN, \"KNN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiO2meaMuCgp"
      },
      "source": [
        "Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFmB5i4AuD7d",
        "outputId": "373a8e63-d48a-41be-e9b1-02be9bdd451c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree: dev accuracy from cross-validation\n",
            "[0.58469945 0.65384615 0.61538462 0.56043956 0.5989011  0.5989011\n",
            " 0.63186813 0.64835165 0.65934066 0.56043956]\n",
            "------------------------------------------------------------------\n",
            "Test Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.58      0.59      3310\n",
            "           1       0.62      0.63      0.63      3610\n",
            "\n",
            "    accuracy                           0.61      6920\n",
            "   macro avg       0.61      0.61      0.61      6920\n",
            "weighted avg       0.61      0.61      0.61      6920\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_DecTree = tree.DecisionTreeClassifier()\n",
        "\n",
        "run_experiment(model_DecTree, \"Decision Tree\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftFUW23Tu3FJ"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPc6wfOOu4d4",
        "outputId": "2d5acdf4-e648-415a-dadd-ae8a5d2cb297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest: dev accuracy from cross-validation\n",
            "[0.64480874 0.68131868 0.65384615 0.71978022 0.64835165 0.68681319\n",
            " 0.64285714 0.69230769 0.72527473 0.69230769]\n",
            "------------------------------------------------------------------\n",
            "Test Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.67      0.66      3310\n",
            "           1       0.69      0.66      0.68      3610\n",
            "\n",
            "    accuracy                           0.67      6920\n",
            "   macro avg       0.67      0.67      0.67      6920\n",
            "weighted avg       0.67      0.67      0.67      6920\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_rf = RandomForestClassifier()\n",
        "\n",
        "run_experiment(model_rf, \"Random Forest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuZfuwddvA15"
      },
      "source": [
        "XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu-uMQhkvFTb",
        "outputId": "81c3eae3-176a-4a19-e808-b58433ec020a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgb_boost: dev accuracy from cross-validation\n",
            "[0.62295082 0.65934066 0.61538462 0.63186813 0.64285714 0.69230769\n",
            " 0.6043956  0.63736264 0.7032967  0.65934066]\n",
            "------------------------------------------------------------------\n",
            "Test Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.61      0.62      3310\n",
            "           1       0.65      0.67      0.66      3610\n",
            "\n",
            "    accuracy                           0.64      6920\n",
            "   macro avg       0.64      0.64      0.64      6920\n",
            "weighted avg       0.64      0.64      0.64      6920\n",
            "\n"
          ]
        }
      ],
      "source": [
        "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
        "\n",
        "scoring = ['accuracy', 'precision_macro', 'recall_macro']\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "scores = cross_validate(xgb_model, X_train, y_train, scoring=scoring, cv=10, return_estimator=True)\n",
        "\n",
        "print(\"xgb_boost: dev accuracy from cross-validation\")\n",
        "print(scores['test_accuracy'])\n",
        "print(\"------------------------------------------------------------------\")\n",
        "\n",
        "#choosing best model\n",
        "max_acc = np.argmax(scores['test_accuracy'])\n",
        "best_model = scores['estimator'][max_acc]\n",
        "\n",
        "#evaluating chosen model on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Test Results:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cajPkdVbw51p"
      },
      "source": [
        "Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb6FYqyK_Dk1"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text clustering\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "Apply the listed clustering methods to the dataset:\n",
        "\n",
        "K means, \n",
        "DBSCAN,\n",
        "Hierarchical clustering. \n",
        "\n",
        "You can refer to of the codes from  the follwing link below. \n",
        "https://www.kaggle.com/karthik3890/text-clustering "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x1l6x6O1He0",
        "outputId": "8a55438d-791e-4cb9-eb9a-1cfad46e9e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'whom', 'was', 'm', \"won't\", 'ourselves', 'be', \"mustn't\", 'being', 're', 'couldn', 'and', \"she's\", \"you'll\", 'there', 'your', 'about', 'of', 'then', 'ain', 'is', 'has', 'does', 'below', 'yourselves', 'more', 'isn', 'very', 'under', 'during', 'o', 'll', \"couldn't\", 'after', 'should', 'now', 'too', 'in', 'hadn', 'my', 'on', 'only', \"aren't\", 'hers', 'for', 'most', 'weren', 'against', 'd', \"doesn't\", 'me', 'ours', \"mightn't\", 'what', 'will', 'to', 'haven', 'doesn', 'from', 'wouldn', 'such', 'once', 'where', 'shouldn', 'am', 'been', 'its', \"wasn't\", 'same', 'but', 'how', 'can', 'our', 'up', 'that', 'by', 'had', 'until', \"you've\", 'doing', 'before', 'she', 'theirs', 'shan', 'who', 'no', 'into', 's', 'with', 'off', 'have', \"hasn't\", 'i', 'needn', \"should've\", 'it', 'an', 'each', \"didn't\", \"hadn't\", 'further', 'are', 'at', 'nor', 'if', 'any', \"you'd\", 't', \"shan't\", 'out', 'hasn', 'not', 'mustn', 'here', 'won', 'him', \"that'll\", 'all', 'itself', 'do', 'this', \"needn't\", 've', 'over', 'mightn', \"weren't\", 'which', 'above', 'we', 'her', 'between', \"don't\", 'they', 'through', 'their', 'themselves', 'while', 'other', 'those', 'than', 'as', 'himself', 'so', 'having', 'again', 'few', 'own', 'he', 'when', 'a', 'y', \"wouldn't\", \"haven't\", 'you', 'didn', 'did', 'because', 'the', 'down', \"isn't\", \"shouldn't\", 'were', 'don', \"you're\", 'why', 'these', 'them', 'yourself', 'some', 'aren', 'both', 'just', 'his', 'herself', 'ma', 'myself', 'wasn', 'yours', \"it's\", 'or'}\n",
            "************************************\n",
            "tasti\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#cleaning functions from https://www.kaggle.com/karthik3890/text-clustering\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "sno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer which is developed in recent years\n",
        "stop=set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "def cleanhtml(sentence): #function to clean the word of any html-tags\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', sentence)\n",
        "    return cleantext\n",
        "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n",
        "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
        "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
        "    return  cleaned\n",
        "print(stop)\n",
        "print('************************************')\n",
        "print(sno.stem('tasty'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to7R2BjQxFXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dfd1f9b-7e31-46ec-f2c3-0b7a8061e3d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/archive.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/archive.zip or\n",
            "        /content/archive.zip.zip, and cannot find /content/archive.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "!unzip \"/content/archive.zip\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/Amazon_Unlocked_Mobile.csv.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWKU6hxQcBft",
        "outputId": "d32b0821-f9a9-4b2a-a6e9-d6c06b97a924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Amazon_Unlocked_Mobile.csv.zip\n",
            "  inflating: Amazon_Unlocked_Mobile.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lQqzohtxU76"
      },
      "outputs": [],
      "source": [
        "m_df = pd.read_csv(\"/content/Amazon_Unlocked_Mobile.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "W25tVdKgxgUJ",
        "outputId": "92060f65-5403-45c5-c059-144d612beebf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Product Name Brand Name   Price  \\\n",
              "0       \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
              "1       \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
              "2       \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
              "3       \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
              "4       \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
              "...                                                   ...        ...     ...   \n",
              "413835  Samsung Convoy U640 Phone for Verizon Wireless...    Samsung   79.95   \n",
              "413836  Samsung Convoy U640 Phone for Verizon Wireless...    Samsung   79.95   \n",
              "413837  Samsung Convoy U640 Phone for Verizon Wireless...    Samsung   79.95   \n",
              "413838  Samsung Convoy U640 Phone for Verizon Wireless...    Samsung   79.95   \n",
              "413839  Samsung Convoy U640 Phone for Verizon Wireless...    Samsung   79.95   \n",
              "\n",
              "        Rating                                            Reviews  \\\n",
              "0            5  I feel so LUCKY to have found this used (phone...   \n",
              "1            4  nice phone, nice up grade from my pantach revu...   \n",
              "2            5                                       Very pleased   \n",
              "3            4  It works good but it goes slow sometimes but i...   \n",
              "4            4  Great phone to replace my lost phone. The only...   \n",
              "...        ...                                                ...   \n",
              "413835       5                     another great deal great price   \n",
              "413836       3                                                 Ok   \n",
              "413837       5        Passes every drop test onto porcelain tile!   \n",
              "413838       3  I returned it because it did not meet my needs...   \n",
              "413839       4  Only downside is that apparently Verizon no lo...   \n",
              "\n",
              "        Review Votes  \n",
              "0                1.0  \n",
              "1                0.0  \n",
              "2                0.0  \n",
              "3                0.0  \n",
              "4                0.0  \n",
              "...              ...  \n",
              "413835           0.0  \n",
              "413836           0.0  \n",
              "413837           0.0  \n",
              "413838           0.0  \n",
              "413839           0.0  \n",
              "\n",
              "[413840 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e44f25a-335d-4709-a002-824e5a443958\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product Name</th>\n",
              "      <th>Brand Name</th>\n",
              "      <th>Price</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Review Votes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>5</td>\n",
              "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>4</td>\n",
              "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>5</td>\n",
              "      <td>Very pleased</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>4</td>\n",
              "      <td>It works good but it goes slow sometimes but i...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>4</td>\n",
              "      <td>Great phone to replace my lost phone. The only...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413835</th>\n",
              "      <td>Samsung Convoy U640 Phone for Verizon Wireless...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>79.95</td>\n",
              "      <td>5</td>\n",
              "      <td>another great deal great price</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413836</th>\n",
              "      <td>Samsung Convoy U640 Phone for Verizon Wireless...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>79.95</td>\n",
              "      <td>3</td>\n",
              "      <td>Ok</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413837</th>\n",
              "      <td>Samsung Convoy U640 Phone for Verizon Wireless...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>79.95</td>\n",
              "      <td>5</td>\n",
              "      <td>Passes every drop test onto porcelain tile!</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413838</th>\n",
              "      <td>Samsung Convoy U640 Phone for Verizon Wireless...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>79.95</td>\n",
              "      <td>3</td>\n",
              "      <td>I returned it because it did not meet my needs...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413839</th>\n",
              "      <td>Samsung Convoy U640 Phone for Verizon Wireless...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>79.95</td>\n",
              "      <td>4</td>\n",
              "      <td>Only downside is that apparently Verizon no lo...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>413840 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e44f25a-335d-4709-a002-824e5a443958')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e44f25a-335d-4709-a002-824e5a443958 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e44f25a-335d-4709-a002-824e5a443958');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "m_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycM5nHj41JDq"
      },
      "outputs": [],
      "source": [
        "#cleaning functions from https://www.kaggle.com/karthik3890/text-clustering\n",
        "i=0\n",
        "str1=' '\n",
        "final_string=[]\n",
        "all_positive_words=[] # store words from +ve reviews here\n",
        "all_negative_words=[] # store words from -ve reviews here.\n",
        "s=''\n",
        "for sent in m_df['Reviews'].values:\n",
        "    filtered_sentence=[]\n",
        "    #print(sent);\n",
        "    sent = str(sent)\n",
        "    sent=cleanhtml(sent) # remove HTMl tags\n",
        "    for w in sent.split():\n",
        "        for cleaned_words in cleanpunc(w).split():\n",
        "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
        "                if(cleaned_words.lower() not in stop):\n",
        "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
        "                    filtered_sentence.append(s)\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                continue \n",
        "    #print(filtered_sentence)\n",
        "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
        "    #print(\"***********************************************************************\")\n",
        "    \n",
        "    final_string.append(str1)\n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZJL1oln0uOX"
      },
      "outputs": [],
      "source": [
        "m_df['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review \n",
        "m_df['CleanedText']=m_df['CleanedText'].str.decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2G3kc3m_Dk1"
      },
      "outputs": [],
      "source": [
        "#Write your code here.\n",
        "sents = list(m_df.loc[:, \"CleanedText\"])\n",
        "\n",
        "sents = [str(s) for s in sents]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vects = vectorizer.fit_transform(sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9dzesvk0myD",
        "outputId": "f42fc05a-e0e0-45a2-f50f-e154ec7bc465"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(413840, 46648)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "vects.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJmLgcTA2ykz"
      },
      "source": [
        "Fitting the clustering algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhw8JesI2x47",
        "outputId": "97920888-dbdf-458f-d308-e2990cec5f36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(n_clusters=10, random_state=99)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "#k means\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "model = KMeans(n_clusters = 10,init='k-means++',random_state=99)\n",
        "model.fit(vects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5X32Rt62-2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b83a9b-e92a-42e9-8072-82b09493ec3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DBSCAN()"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "#DBSCAN\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "model = DBSCAN()\n",
        "\n",
        "model.fit(vects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGdayQ_g3Ic1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "5c3580f7-1e3d-4026-8619-1d835325b634"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-444c8976a1f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgglomerativeClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vects' is not defined"
          ]
        }
      ],
      "source": [
        "#heirarchical\n",
        "#this one will not run because my system does not have enough RAM so my code breaks every time after this\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "model = AgglomerativeClustering()\n",
        "vects = vects.toarray()\n",
        "model.fit(vects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9A1g1Xp_Dk2"
      },
      "source": [
        "In one paragraph, please compare K means, DBSCAN and Hierarchical clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df4nVK0swtKJ"
      },
      "source": [
        "K means is a partitional clustering algorithm, while DBSCAN is a density based clustering algorithm, and Heirarchical clustering is an agglomerative based clustering method. K means is useful because it is fast and efficient, but the desired number of clusters must be specified beforehand. In K means, points are assigned to the cluster for which they are closest to the centroid. As points are reassigned, the centroids are recalculated, until the clusters stabilize. K means is not as good at handling outliers or clusters with strange shapes. DBSCAN is good for handling outliers and strange shaped clusters, and the number of clusters does not need to be specified beforehand. Clusters are calculated by grouping points together which all have a certain number of neighbors within a specified radius. Hierarchical Clustering uses linking distance to merge things into the same cluster. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "megan_in_class_exercise_05_11172022.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}